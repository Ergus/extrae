.\" Process this file with
.\" groff -man -Tascii mpitrace.1
.\"
.TH MPITRACE 1 "FEBRUARY 2007"

.SH NAME
mpitrace \- MPI tracing facility for the Paraver visualizer tool
.\"
.\" Com va?
.\" 
.SH SYNOPSIS
.B [ld/cc] ... -lmpitrace
.B mpitrace mpirun [mpirun options] application

or, 

.B export LD_PRELOAD=${MPITRACE_HOME}/lib/libmpitrace.so
.\"
.\" Descripcio
.\"
.SH DESCRIPTION
The 
.B MPItrace
tool instruments parallel codes based on the message passing (MPI) programming model. This tool generates intermediate files which must be merged by mpi2prv tool to obtain a final Paraver tracefile. In this tracefile, all the MPI activity plus other added manually at the source code of the target application. On the Linux/PPC with XL compilers, some OpenMP activity will be recorded.

There are two kinds of use of the tracing tool.

For a basic use of the tracing tool, just link your MPI application with the MPItrace library (use -L... and -l at the link stage). When the application is linked with this library then the application is ready to generate all the information required to generate a Paraver tracefile. If you link with a tracing library that is capable to obtain information from PAPI library, then you're required to link the application with -L${PAPI_HOME}/lib -lpapi. Furthermore, latest tracing libraries can read the configuration from an XML file. To be able to link with this library just add libxml2 to your link stage using -lxml2 flag (because libxml2 is usually installed on the default library path).

  In brief, link your application in this way

  $(LINKER) -o appName *.o -L${MPITRACE_HOME}/lib -lmpitrace -L${PAPI_HOME}/lib -lpapi -lxml2

However, if your machine does support LD_PRELOAD (ELF binaries) dynamic load, just define this variable to point the shared tracing library (libmpitrace.so, i.e.) before running the application, instead of linking your application with the MPItrace library. In this way, you must setup LD_LIBRARY_PATH in a such way that PAPI libraries can be found if the tracing library uses them. 

To enable the tracing, take a look on the environment variables.

In some environments there could also be more library dependencies. For example, to obtain information of the hardware counter then the PAPI library should be added to the linking stage. Sometimes, the MPI library does not contain the PMPI symbols and an additional library must be used (usually -lpmpi).

Once the application has ended succesfully, several intermediate (several .mpit files and one .mpits) files will be generated. All these files can be used at the merge time to generate the final Paraver tracefile. Look at the mpi2prv(1) man page to know how.

To run the instrumented application the mpitrace frontend can be used. Just launch it with the mpirun and mpirun's parameter, no other parameter is required.

This tool has been ported to: Linux/IA32, Linux/IA64, Linux/amd64, Linux/PPC, Linux/BGL, DEC.
.SH ENVIRONMENT
This section describes all the environment variables related to the mpitrace tracing library.
.IP "MPITRACE_BASE"
This environment variable is deprecated. Please, do not use it.
.IP "MPTRACE_BUFFER_SIZE"
Set the number of events that each trace buffer can hold. By default, its value is 100k events. This value can be modified to achieve some memory constraints (reduce it so as to each task use lesser memory).

When the buffers are full they're sent to the disk, so if the buffer is very small then many writes can occur.
.IP "MPTRACE_CONFIG_FILE"
Provide a XML configuration file (see 
.B ${MPITRACE_HOME}/share/example/mpitrace_explained.xml
) to setup the tracing facility instead of using environment variables.
.IP "MPTRACE_CONTROL_FILE"
Specifies the file that will be used as a control of the tracing. While the file does not exist the tracing will be disabled. It will be activated once the file is created. See MPTRACE_CONTROL_TIME for more information on the behaviour of this environment variable.
.IP "MPTRACE_CONTROL_GLOPS"
Specifies which global operation pattern (i.e., MPI_Reduce, MPI_Bcast, and so on, based on MPI_COMM_WORLD) will control (enable/disable) the tracing package. In order to do that the tracing package identifies the order of such operations with a cardinal, and the user must provide a list of pairs where the first value identifies the global operation that will enable the tracing, and the latter identifies the operation that disables the tracing. For example,

MPTRACE_CONTROL_GLOPS=3-5,8-10,20

Will enable the tracing when the 3rd global op call (on MPI_COMM_WORLD) arribes, and disable it at the 5th MPI global op call. The same applies to calls number 8th and 10th. Once application reaches the 20th global op call, the tracing will be enabled and will deactived by this method. If you don't specify a single value (i.e. 20 in this example), the tracing will be shutdown once the global operation identified by the latter value of the last pair is executed.
.IP "MPTRACE_CONTROL_TIME"
If set, this variable controls the frequency for the check of the control file after a warmup of 100 global operations performed on MPI_COMM_WORLD. If not set, the control file will be checked every 100 global operations on MPI_COMM_WORLD. Time can be set with s[econds],m[inutes],h[ours],d[ays] suffix, defaulting to seconds. For example, MPTRACE_MINIMUM_TIME=30m will trace for at least 30 minutes.
.IP "MPTRACE_DIR"
Specifies which directory should be used for storing the temporal files. If this variable is not defined, then the working directory is used instead.
.IP "MPTRACE_FINAL_DIR"
Specifies where the final trace must be stored. If this variable is not defined, then MPTRACE_DIR is used instead.
.IP "MPTRACE_GATHER_MPITS"
Set this variable to 1 to gather all the mpits in the master node when the application executes MPI_Finalize. Final directory in the master node can be configured through MPTRACE_DIR and/or MPTRACE_FINAL_DIR environment variables.
.IP "MPITRACE_INITIAL_MODE"
Selects the initial tracing mode. Valid values are "detail" and "bursts".
.IP "MPITRACE_BURST_THRESHOLD"
Specifies when a burst must be discarded when using the bursts library.
.IP "MPITRACE_DISABLE_MPI"
Set this variable if MPI calls must not be emitted to the tracefile. This option can be useful when the application has some tracing calls (like mpitrace_event or mpitrace_eventandcounters) but there's no need to know a detailed of the MPI calls (and thus reducing the size of the final tracefile).
.IP "MPITRACE_DISABLE_OMP"
Set this variable if OpenMP calls must not be emitted to the tracefile. This option can be useful when the application has some tracing calls (like mpitrace_event or mpitrace_eventandcounters) and MPI calls but there's no need to know a detailed of the OpenMP calls (and thus reducing the size of the final tracefile).

*THIS* environment variable is only available for Linux/PPC currently.
.IP "MPITRACE_HOME"
Defines where the mpitrace tracing facility is installed. Under ${MPITRACE_HOME} there must exist the following directories: bin/, etc/, lib/, share/example, and share/etc.

This environment variable must be set *ALWAYS*.
.IP "MPITRACE_MPI_CALLER"
MPItrace can obtain the routine that executed each MPI call. The binary 
.B must be compiled with debug information
(usually -g flag at 
.B compile and link
time) in order to convert the routine adresses into routine names. This environment variable let's the user choose which depth of the frame stack must be tacken as the caller routine (or routines).

For example, if the user want to know which routine made a direct call to any MPI routine, just set MPITRACE_MPI_CALLER to 1.

If the user wants to know up to three levels of calls of any MPI routine, just set MPITRACE_MPI_CALLER to 1-3 or 1,2,3.

In order to obtain the textual information about these functions in your final trace file, remember to add the -e parameter at the merge step.

.B Fortran applications must care about when using LD_PRELOAD, MPI callers are shifted 1 level up, so if the user intend to obtain levels 1-3, he or she must place values 2-4 in the MPITRACE_MPI_CALLER environment variable.
.IP "MPITRACE_MPI_STATISTICS"
The burst tracing library can provide information of how many MPI p2p/global operations and some valuable statistics of what have been ocurred before emitting the running burst.
.IP "MPTRACE_CIRCULAR_BUFFER"
Set this environment variable if internals buffers must be used in a circular way. This method allows the user trace a very large run saving only the last MPTRACE_BUFFER_SIZE events. For the merging process, this option adds information on several MPI collective calls. Nowadays, these calls are: MPI_Barrier, MPI_Bcast, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv and MPI_Scan.
.IP "MPTRACE_COUNTERS"
Indicates which hardware counters will be taken at different points (see MPITRACE_MPI_COUNTERS_ON). Right now, this environment variable is a comma-separated list of identifiers given by PAPI (both PAPI and native counter identifiers). On BG/L systems the identifiers are strings (like PAPI_TOT_CYC) while on Linux/* systems the identifiers are 32bit values.

You can find more information on PAPI homepage (http://icl.cs.utk.edu/papi). Information for the available hardware counters are on http://icl.cs.utk.edu/projects/papi/presets.html
.IP "MPTRACE_COUNTERS_DOMAIN"
PAPI can gather counter information at different levels, i.e. User level, Kernel level among others. The tracing facility can configure PAPI in such way. Just set this variable to USER, KERNEL or ALL to gather information of such level.
.IP "MPTRACE_COUNTERS_VERBOSE"
Set it to enable a verbose output of the hardware counters.
.IP "MPTRACE_FILE_SIZE"
Set it to limit the size of the intermediate files. For example, set to 5 if all the intermediate files must be limited up to 5 Mbytes. As this check is done each time the buffer is written to the disk, intermediate files can be slightly greater than value defined in this environment variable. By default, MPTRACE_BUFFER_SIZE is 100k and each time the buffer is written will need about 6Mbytes.
.IP "MPTRACE_FUNCTIONS"
Points to an ASCII file containing a list of user functions to be traced (one function per line).
Right now, the only way to instrument user functions is to compile the target program with the -finstrument-functions (only available with GCC).
In order to obtain the textual information about these functions in your final trace file, remember to
.B compile and link your application with debug information
, and add the -e parameter at the merge step.
.B See UFlist(1) to obtain further information on how to obtain a list of instrumentable routines.
.IP "MPTRACE_FUNCTIONS_COUNTERS_ON"
Set this variable if the selected user functions must take an snapshot of the hardware counters. 
Note that if MPITRACE_OMP_COUNTERS_ON, MPITRACE_MPI_COUNTERS_ON and MPTRACE_FUNCTIONS_COUNTERS_ON are not set, then the hardware counters will only appear when a call to mpitrace_counters, mpitrace_eventandcounters happens.
.IP "MPTRACE_FUNCTIONS_MAX_DEPTH"
Limit the depth of the callgraph of the selected user functions. If this variable is set, not all the selected user functions will appear, only those that are on the top of the callgraph tree.
If this variable is not set, instrumentation package will emit information for all the selected routines with MPTRACE_FUNCTIONS variable. 
.IP "MPTRACE_MINIMUM_TIME"
Choose the minimum tracing time for the application. This option supersede MPTRACE_FILE_SIZE while the application does not consume this amount of time. This check will be done once per flush. Time can be set with s[econds],m[inutes],h[ours],d[ays] suffix, defaulting to seconds. For example, MPTRACE_MINIMUM_TIME=30m will trace for at least 30 minutes.
.IP "MPITRACE_NETWORK_COUNTERS"
Obtain network counters for the traced application if this variable is set to 1. Such counters depends directly from the network device.

Nowadays, this function is only supported on Myrinet GM/MX devices.
.IP "MPITRACE_PROGRAM_NAME"
Indicates which is the "name" of the program that must be run. This name is used only for naming the intermediate files. If this variable is not supplied, then all the generated files will be called TRACE*.mpit. This may lead to some confusion if several runs send the output files to the same directory (see MPTRACE_DIR).
.IP "MPITRACE_MPI_COUNTERS_ON"
Set this variable if all the MPI calls must take an snapshot of the hardware counters. This call is useful for obtaining some hardware counters without modifying the source code, but the information obtained has some limitations: the hardware counters are *ONLY* present (except if OpenMP runtime or some user functions are traceable) when a MPI call occurs and the tracefile can be larger (in fact, it depends on how many MPI calls are invoked in the application).

Note that if MPITRACE_OMP_COUNTERS_ON, MPITRACE_MPI_COUNTERS_ON and MPTRACE_FUNCTIONS_COUNTERS_ON are not set, then the hardware counters will only appear when a call to mpitrace_counters, mpitrace_eventandcounters happens.
.IP "MPITRACE_OMP_COUNTERS_ON"
Set this variable if all the OpenMP calls must take an snapshot of the hardware counters. 

Note that if MPITRACE_OMP_COUNTERS_ON, MPITRACE_MPI_COUNTERS_ON and MPTRACE_FUNCTIONS_COUNTERS_ON are not set, then the hardware counters will only appear when a call to mpitrace_counters, mpitrace_eventandcounters happens.

*THIS* environment variable is only available for Linux/PPC currently.
.IP "MPTRACE_OMP_LOCKS"
If the tracing library can instrument OpenMP set this environment variable to trace lock functions. This provides a more detailed information of the run, but the final tracefil can be much larger. 
.IP "MPITRACE_ON"
Set this variable if the tracing must be enabled. Unset it if the tracing is not wanted for that run.
.IP "MPITRACE_RUSAGE"
Setting this variable to 1 will emit in the tracefile information about resource usage information (see getrusage man page).
.IP "MPITRACE_SIGNAL_FLUSH_TERMINATE"
Set this variable to USR1 or USR2 if tracing facility must flush the buffers when SIGUSR1/SIGUSR2 is received by a *COMPUTE* process.
.IP "MPTRACE_THRESHOLD"
This environment variable is deprecated. Please, do not use it.
.SH EXAMPLES
Just to link the target application with the tracing facility just use one of the following approaches:

gcc input.c -o out -L${MPITRACE_HOME}/lib -lmpitrace -lxml2

g77 input.f -o out -L${MPITRACE_HOME}/lib -lmpitracef -lxml2

If hardware counters are enabled in your package, you must add some performance counter libraries:

on PAPI systems - gcc input.c -o out -L${MPITRACE_HOME}/lib -lmpitrace -L${PAPI_HOME}/lib -lpapi -lxml2
on BGL  systems - gcc input.c -o out -L${MPITRACE_HOME}/lib -lmpitrace.rts -L${PAPI_HOME}/lib -lpapi.rts -lbglperfctr.rts

On DEC machines MPI_* symbols and PMPI_* symbols are sepparatd in two files. You could link the application with:

cc input.c -o out -L${MPITRACE_HOME}/lib -lmpitrace -lpmpi -lmpi

In order to run the instrumented application, use mpitrace binary:

mpitrace mpirun -np ${NUMBER_OF_TASKS} ./out

To use the LD_PRELOAD variant, just compile and link the application as always, and make sure to set the LD_PRELOAD environment variable to point the libmpitrace.so (or libompitrace.so if OpenMP support is requested and supported).

In TCSH/CSH:

setenv LD_PRELOAD ${MPITRACE_HOME}/lib/libmpitrace.so

In BASH/KSH/SH:

export LD_PRELOAD=${MPITRACE_HOME}/lib/libmpitrace.so
.SH AUTHOR
Tool was developed at the CEPBA (European Center for Parallelism of Barcelona) by the "tools team".

This online documentation was written by Harald Servat Gelabert.
.SH "REPORTING BUGS"
If you find any bug on the documentation or in the software, please send a descriptive mail to

.B cepbatools@cepba.upc.edu
.SH "ONLINE DOCUMENTATION"
More information and detailed examples can be found at

.B http://www.cepba.upc.edu/paraver/
.SH "SEE ALSO"
.BR mpi2prv (1)
.BR UFlist (1)

.BR mpitrace_event (3)
.BR mpitrace_counters (3)
.BR mpitrace_eventandcounters (3)
.BR mpitrace_shutdown (3)
.BR mpitrace_restart (3)
.BR mpitrace_set_tracing_tasks (3)
.BR mpitrace_set_options (3)
